# Indoor Guide Drone for Visually Impaired People

## Project Overview
The Indoor Guide Drone project is an exploratory endeavor aimed at enhancing indoor navigation for visually impaired individuals. Utilizing advanced technologies like real-time image processing, voice recognition, and autonomous drone navigation, this project seeks to provide a novel solution for spatial awareness and navigational assistance.

## Features
- **Autonomous Navigation**: Using ArUco marker tracking for seamless user following.
- **Voice Command Interface**: Intuitive control through natural language voice commands.
- **Real-Time Image Processing**: Accurate and timely processing for marker tracking and environment mapping.
- **Collision Detection**: Using accelerometer data to sense and respond to obstacles.
- **Auditory Feedback**: Text-to-speech system for communicating information about surroundings.

## Installation
1. Clone the repository:
git clone [Repository URL]

2. Install required libraries.
  - OpenCV
  - djitellopy
  - requests
  - speech_recognition
  - pygame

## Usage
- Ensure the drone is connected and the camera is properly configured.
- Run the main script to start the drone's operation:
DroneNavigationMain.py
- Use voice commands like 'start', 'stop', 'start tracking' and 'check for surrounding' to interact with the drone.

## Contributing
Contributions to this project are welcome. Please fork the repository and submit a pull request with your proposed changes.

## Acknowledgments
Special thanks to Professor Ilmi Yoon for guidance and support throughout this project.

## Contact
For more information, please contact Tanishq Pradhan at tanishq97.tp@gmail.com.
